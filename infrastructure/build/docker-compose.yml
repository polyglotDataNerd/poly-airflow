version: '3'
# https://www.techrepublic.com/article/what-is-the-difference-between-dockerfile-and-docker-compose-yml-files/
# https://docs.docker.com/compose/compose-file/
# https://medium.com/@xnuinside/quick-guide-how-to-run-apache-airflow-cluster-in-docker-compose-615eb8abd67a
# docker-compose config: runs a test to see if all vars were populated
services:
  postgres:
    image: postgres:9.6
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data

  webserver:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        AWS_ACCESS_KEY_ID: "${AWS_ACCESS}"
        AWS_SECRET_ACCESS_KEY: "${AWS_SECRET}"
        GitToken: "${GitToken}"
        AIRFLOW__CORE__REMOTE_LOGGING:  $AIRFLOW__CORE__REMOTE_LOGGING
        AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER:  $AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER
        AIRFLOW__CORE__REMOTE_LOG_CONN_ID:  $AIRFLOW__CORE__REMOTE_LOG_CONN_ID
        AIRFLOW__CORE__ENCRYPT_S3_LOGS:  $AIRFLOW__CORE__ENCRYPT_S3_LOGS
        AIRFLOW__CORE__EXECUTOR: $AIRFLOW__CORE__EXECUTOR
        AIRFLOW__CORE__SQL_ALCHEMY_CONN: $AIRFLOW__CORE__SQL_ALCHEMY_CONN
    image: "${image}"
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
    restart: always
    depends_on:
      - postgres
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./dags:/usr/local/airflow/dags
      # - ./plugins:/usr/local/airflow/pluginsy
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
